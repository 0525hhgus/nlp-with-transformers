{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe-zJgwQvy6J"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/nlp-with-transformers/blob/main/10_transformers-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"코랩에서 실행하기\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**코랩을 사용하는 경우 디스크 공간이 부족하기 때문에 코랩 프로(https://colab.research.google.com/signup) 를 사용해야 합니다.**"
      ],
      "metadata": {
        "id": "GImATsXEJfoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_L31Ayz7vy6N",
        "outputId": "38da632e-6750-4f52-a267-69c17f526d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nlp-with-transformers' already exists and is not an empty directory.\n",
            "/content/nlp-with-transformers\n",
            "⏳ Installing base requirements ...\n",
            "✅ Base requirements installed!\n",
            "⏳ Installing Git LFS ...\n",
            "✅ Git LFS installed!\n"
          ]
        }
      ],
      "source": [
        "# 코랩이나 캐글을 사용한다면 이 셀의 주석을 제거하고 실행하세요.\n",
        "!git clone https://github.com/rickiepark/nlp-with-transformers.git\n",
        "%cd nlp-with-transformers\n",
        "from install import *\n",
        "install_requirements(is_chapter10=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OAFVc9arvy6P",
        "outputId": "ca780e24-c644-44b2-9a2a-0880115b9834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU was detected! This notebook can be *very* slow without a GPU 🐢\n",
            "Go to Runtime > Change runtime type and select a GPU hardware accelerator.\n",
            "Using transformers v4.11.3\n",
            "Using datasets v1.16.1\n"
          ]
        }
      ],
      "source": [
        "from utils import *\n",
        "setup_chapter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AkGR1Dcvy6P"
      },
      "source": [
        "# 밑바닥부터 트랜스포머 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZIbkmCvy6Q"
      },
      "source": [
        "> **Note:** 이 장에서는 분산 인프라에서 대규모 언어 모델을 훈련하기 위한 대용량 데이터셋과 스크립트를 만듭니다. 따라서 이 노트북의 모든 단계가 코랩이나 캐글 같은 플랫폼에서 실행 가능한 것은 아닙니다. 중요 지점에서 단계를 축소하거나 분산 훈련 스크립트를 만들 때 참고용으로 이 노트북을 사용하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO-eUJJxvy6Q"
      },
      "source": [
        "## 대규모 데이터셋 수집하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecriu92Rvy6Q"
      },
      "source": [
        "### 대규모 말뭉치 구축의 어려움"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QJRlTU94vy6R"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
        "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XYdtdz7bvy6R",
        "outputId": "3bb15f00-97a2-4464-92a8-8c09c3c9e218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT  크기: 116.5M parameters\n",
            "GPT2 크기: 124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "def model_size(model):\n",
        "    return sum(t.numel() for t in model.parameters())\n",
        "\n",
        "print(f\"GPT  크기: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
        "print(f\"GPT2 크기: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9eu8w_hfvy6S"
      },
      "outputs": [],
      "source": [
        "set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jjU2Ge9kvy6T",
        "outputId": "b1b0ad9d-fcb5-46dd-9b40-7e0dab668600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT 자동 완성:\n",
            "1.\n",
            "When they came back.\n",
            " \" we need all we can get, \" jason said once they had settled into the back of\n",
            "the truck without anyone stopping them. \" after getting out here, it 'll be up\n",
            "to us what to find. for now\n",
            "2.\n",
            "When they came back.\n",
            " his gaze swept over her body. he 'd dressed her, too, in the borrowed clothes\n",
            "that she 'd worn for the journey.\n",
            " \" i thought it would be easier to just leave you there. \" a woman like\n",
            "3.\n",
            "When they came back to the house and she was sitting there with the little boy.\n",
            " \" don't be afraid, \" he told her. she nodded slowly, her eyes wide. she was so\n",
            "lost in whatever she discovered that tom knew her mistake\n",
            "\n",
            "GPT-2 자동 완성:\n",
            "1.\n",
            "When they came back we had a big dinner and the other guys went to see what\n",
            "their opinion was on her. I did an hour and they were happy with it.\n",
            "2.\n",
            "When they came back to this island there had been another massacre, but he could\n",
            "not help but feel pity for the helpless victim who had been left to die, and\n",
            "that they had failed that day. And so was very, very grateful indeed.\n",
            "3.\n",
            "When they came back to our house after the morning, I asked if she was sure. She\n",
            "said, \"Nope.\" The two kids were gone that morning. I thought they were back to\n",
            "being a good friend.\n",
            "\n",
            "When Dost\n"
          ]
        }
      ],
      "source": [
        "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
        "    out = pipe(prompt, num_return_sequences=num_return_sequences,\n",
        "               clean_up_tokenization_spaces=True)\n",
        "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
        "\n",
        "prompt = \"\\nWhen they came back\"\n",
        "print(\"GPT 자동 완성:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\n",
        "print(\"\")\n",
        "print(\"GPT-2 자동 완성:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWvANmfzvy6U"
      },
      "source": [
        "### 사용자 정의 코드 데이터셋 만들기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LiWpJv1vy6V"
      },
      "source": [
        "#### 구글 빅쿼리로 데이터셋 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**깃허브를 클론하면 .git 폴더가 디스크를 많이 차지하므로 대신 수동으로 파일을 하나씩 다운로드합니다.**"
      ],
      "metadata": {
        "id": "C7dQjfZ7trYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://huggingface.co/datasets/transformersbook/codeparrot\n",
        "# %cd codeparrot\n",
        "# !git lfs pull\n",
        "# %cd .."
      ],
      "metadata": {
        "id": "dPQ8bPtwyH2G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir codeparrot\n",
        "%cd codeparrot\n",
        "for i in range(184):\n",
        "    no = f'{i:03d}'\n",
        "    !wget -q https://huggingface.co/datasets/transformersbook/codeparrot/resolve/main/file-000000000{no}.json.gz\n",
        "%cd .."
      ],
      "metadata": {
        "id": "SEoW1vVAqnfe",
        "outputId": "67b3dd89-0468-4848-cfe5-82e7c0c2bac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘codeparrot’: File exists\n",
            "/content/nlp-with-transformers/codeparrot\n",
            "/content/nlp-with-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s0JZFB4vy6V"
      },
      "source": [
        "#### 사이드바: 잡음 필터링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDbFw-1Qvy6V"
      },
      "source": [
        "### 대용량 데이터셋 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_v-QVvQvy6W"
      },
      "source": [
        "#### 메모리 매핑"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9ooPAL0vy6W"
      },
      "source": [
        "> **Note:** 다음 코드 블록은 빅쿼리 데이터셋을 `codeparrot` 폴더에 다운로드했다고 가정합니다. 압축 파일을 풀면 ~180GB 디스크 공간이 필요하기 때문에 이 단계를 건너 뛰는 것이 좋습니다. 이 코드는 예시 목적으로 쓴 것입니다. 대신 디스크 공간을 많이 사용하지 않는 스트리밍 데이터셋을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLSs9XeIvy6W"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset, DownloadConfig\n",
        "\n",
        "# download_config = DownloadConfig(delete_extracted=True)\n",
        "# dataset = load_dataset(\"./codeparrot\", split=\"train\",\n",
        "#                        download_config=download_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeOsgwqovy6X"
      },
      "outputs": [],
      "source": [
        "# import psutil, os\n",
        "\n",
        "# print(f\"데이터셋에 있는 파이썬 파일의 개수 : {len(dataset)}\")\n",
        "# ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
        "# # os.stat.st_size는 바이트 단위이므로 GB로 바꿉니다\n",
        "# print(f\"데이터셋 크기 (캐시 파일) : {ds_size / 2**30:.2f} GB\")\n",
        "# # Process.memory_info는 바이트 단위이므로 MB로 바꿉니다\n",
        "# print(f\"메모리 사용량 : {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpO-8WM3vy6X"
      },
      "source": [
        "#### 스트리밍"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "TB2qYwg_zWBg",
        "outputId": "76c319de-343d-438a-9643-6ac6770e222f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.16.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: responses, datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 1.16.1\n",
            "    Uninstalling datasets-1.16.1:\n",
            "      Successfully uninstalled datasets-1.16.1\n",
            "Successfully installed datasets-2.4.0 responses-0.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nlp-with-transformers/"
      ],
      "metadata": {
        "id": "eY9xiG5KzgDG",
        "outputId": "26f3733c-f2a3-496f-dd7a-df694f786fe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlp-with-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ca0cCULJvy6X",
        "outputId": "3e93d0bb-0d78-4bf9-b661-9b028748552d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c7097ae8aaa84c59ba42093dde5d1703",
            "742bfe3ce6ca4fde9769ad3996b4b5a0",
            "ef8b2387a73a4b3b875563fdd3a3d390",
            "9bf683804d314145aee648f38b1eea63",
            "815c8ebf4adf41b2a605846b7329d50f",
            "d1e0352d6c1f4978b779a01da7d10a01",
            "68e06296b5df4202afd7c3d796f29a55",
            "f159b1c93b584542863fe41e3462e5b8",
            "29e9e5c12d564fb38e28a7ac9bd11f21",
            "b5a860fce21f4a629f1ccc8fc03931cb",
            "8df2464a04e341bfa1ba8b5ab7a3b93a"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/186 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7097ae8aaa84c59ba42093dde5d1703"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration codeparrot-b510235f58772661\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "streamed_dataset = load_dataset('./codeparrot', split=\"train\", streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hhExG0xSvy6Y"
      },
      "outputs": [],
      "source": [
        "# iterator = iter(streamed_dataset)\n",
        "\n",
        "# print(dataset[0] == next(iterator))\n",
        "# print(dataset[1] == next(iterator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0I3-3wdbvy6Y",
        "outputId": "5fbc8c5e-2f37-45af-fcfd-bc6d98e0d803",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration transformersbook--codeparrot-a329ea5130797998\n"
          ]
        }
      ],
      "source": [
        "remote_dataset = load_dataset('transformersbook/codeparrot', split=\"train\",\n",
        "                              streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(streamed_dataset)\n",
        "remote_iterator = iter(remote_dataset)\n",
        "\n",
        "print(next(remote_iterator) == next(iterator))\n",
        "print(next(remote_iterator) == next(iterator))"
      ],
      "metadata": {
        "id": "zWmNubcazzwX",
        "outputId": "888ed2bd-629c-488c-8ab6-0b4b34b079c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-106kfxvy6Y"
      },
      "source": [
        "### Adding Datasets to the Hugging Face Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed3Xj4J7vy6Z"
      },
      "source": [
        "## Building a Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqJr-1-mvy6Z",
        "outputId": "33bc364d-d9ad-452a-ab4c-59546153f977",
        "colab": {
          "referenced_widgets": [
            "29ced71e91434126970160a03cc006a5",
            "6f437f06babc4f01b5f02ed2e11a274f",
            "086fad17475145a2960cf393d6da4da5",
            "83b90218ddd54563b5abc524ed820741",
            "8fb62d059f0a4ab397767ec6497ab5ed",
            "b1fd604f95db44748a1b53a31be9ff5a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29ced71e91434126970160a03cc006a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f437f06babc4f01b5f02ed2e11a274f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "086fad17475145a2960cf393d6da4da5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83b90218ddd54563b5abc524ed820741",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fb62d059f0a4ab397767ec6497ab5ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1fd604f95db44748a1b53a31be9ff5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def tok_list(tokenizer, string):\n",
        "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
        "    return [tokenizer.decode(tok) for tok in input_ids]\n",
        "\n",
        "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGxgWwGQvy6Z",
        "outputId": "bdf486fd-d62f-49fd-a1ff-60665bff934d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T5 tokens for \"sex\": ['', 's', 'ex']\n",
            "CamemBERT tokens for \"being\": ['be', 'ing']\n"
          ]
        }
      ],
      "source": [
        "print(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\n",
        "print(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcFrpxtwvy6a"
      },
      "source": [
        "### The Tokenizer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrFTTRUXvy6a"
      },
      "source": [
        "### Measuring Tokenizer Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb0VbzQPvy6a"
      },
      "source": [
        "### A Tokenizer for Python "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRAKsdkMvy6a",
        "outputId": "bfd278fc-c090-43dc-dbbb-48a351919ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"',\n",
            "'Hello', ',', 'ĠWorld', '!\"', ')', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_',\n",
            "'hello', '()', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "python_code = r\"\"\"def say_hello():\n",
        "    print(\"Hello, World!\")\n",
        "# Print it\n",
        "say_hello()\n",
        "\"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "print(tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvLFNGSbvy6b",
        "outputId": "230ba6bb-6bbf-4057-d910-343d9b5c8e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5VWd3YYvy6b",
        "outputId": "7c0daa8c-8b38-4d6b-8b92-1ccd2cfff5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():',\n",
            "(13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello',\n",
            "(28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ġ#', (43,\n",
            "45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('Ċ', (55, 56)),\n",
            "('say', (56, 59)), ('_', (59, 60)), ('hello', (60, 65)), ('()', (65, 67)), ('Ċ',\n",
            "(67, 68))]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT4AhMg1vy6c",
        "outputId": "21657ac0-7d66-49a1-bb20-bffbe0be35ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`a` is encoded as `b'a'` with a single byte: 97\n",
            "`€` is encoded as `b'\\xe2\\x82\\xac'` with three bytes: [226, 130, 172]\n"
          ]
        }
      ],
      "source": [
        "a, e = u\"a\", u\"€\"\n",
        "byte = ord(a.encode(\"utf-8\"))\n",
        "print(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\n",
        "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
        "print(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14itVG-Yvy6c",
        "outputId": "fb55a579-ccd3-4224-daf9-25f10a7000b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of our base vocabulary: 256\n",
            "First element: `!`, last element: `Ń`\n"
          ]
        }
      ],
      "source": [
        "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
        "\n",
        "byte_to_unicode_map = bytes_to_unicode()\n",
        "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "base_vocab = list(unicode_to_byte_map.keys())\n",
        "\n",
        "print(f'Size of our base vocabulary: {len(base_vocab)}')\n",
        "print(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJKxP1bevy6c",
        "outputId": "34db0fe2-a6ef-492b-ba59-ee4576b4984e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Description</th>\n",
              "      <th>Character</th>\n",
              "      <th>Bytes</th>\n",
              "      <th>Mapped bytes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Regular characters</td>\n",
              "      <td>`a` and `?`</td>\n",
              "      <td>97 and 63</td>\n",
              "      <td>`a` and `?`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Non-printable control character (CARRIAGE RETURN)</td>\n",
              "      <td>`U+000D`</td>\n",
              "      <td>13</td>\n",
              "      <td>`č`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>A space</td>\n",
              "      <td>` `</td>\n",
              "      <td>32</td>\n",
              "      <td>`Ġ`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>A non-breakable space</td>\n",
              "      <td>`\\xa0`</td>\n",
              "      <td>160</td>\n",
              "      <td>`ł`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>A newline character</td>\n",
              "      <td>`\\n`</td>\n",
              "      <td>10</td>\n",
              "      <td>`Ċ`</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#id unicode_mapping\n",
        "#caption Examples of character mappings in BPE\n",
        "import pandas as pd\n",
        "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
        "\n",
        "byte_to_unicode_map = bytes_to_unicode()\n",
        "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "base_vocab = list(unicode_to_byte_map.keys())\n",
        "\n",
        "examples = [\n",
        "    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n",
        "    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n",
        "    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n",
        "    ['A nonbreakable space', '`\\\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n",
        "    ['A newline character', '`\\\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n",
        "]\n",
        "\n",
        "pd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1PbVZ62vy6d",
        "outputId": "564d7e60-3f30-4fdf-eace-e2b64694f4dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():',\n",
            "(13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello',\n",
            "(28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ġ#', (43,\n",
            "45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('Ċ', (55, 56)),\n",
            "('say', (56, 59)), ('_', (59, 60)), ('hello', (60, 65)), ('()', (65, 67)), ('Ċ',\n",
            "(67, 68))]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Jtm9BJvy6d",
        "outputId": "e629148a-59c4-4226-8112-3e5ef2d651bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the vocabulary: 50257\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of the vocabulary: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32XrsQHzvy6d",
        "outputId": "b9ec4950-8562-4aca-dea1-d567e5e9ce8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"',\n",
            "'Hello', ',', 'ĠWorld', '!\"', ')', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_',\n",
            "'hello', '()', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpxswRzsvy6d"
      },
      "source": [
        "### Training a Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMlbpKotvy6e",
        "outputId": "1cc5b624-db77-4ab5-b65f-0b3fde9b7ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', '\n",
            "=================================================================', '\n",
            "----------------------------------------------------------------',\n",
            "'................................................................',\n",
            "'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ',\n",
            "'----------------------------------------------------------------',\n",
            "'================================================================',\n",
            "'________________________________________________________________']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:8]]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGDZ2fVtvy6e",
        "outputId": "89183191-f5e4-43ae-e49d-449a4a0a22f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated',\n",
            "' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:12]]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggh38E4Kvy6e",
        "outputId": "78748514-7e12-4df5-8c0c-767b6f1f4421",
        "colab": {
          "referenced_widgets": [
            "743bca69d71649908db9ca5760af61d2"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "743bca69d71649908db9ca5760af61d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Check remote data files:   0%|          | 0/183 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration codeparrot-train-99775fd6743284b5\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "length = 100000\n",
        "dataset_name = 'transformersbook/codeparrot-train'\n",
        "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
        "iter_dataset = iter(dataset)\n",
        "\n",
        "def batch_iterator(batch_size=10):\n",
        "    for _ in tqdm(range(0, length, batch_size)):\n",
        "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
        "\n",
        "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), \n",
        "                                                  vocab_size=12500,\n",
        "                                                  initial_alphabet=base_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma-9Lyuyvy6f",
        "outputId": "4a9dcf07-b670-4c6f-9eb7-e5844d1dd059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['  ', '    ', '   ', '        ', 'se', 'in', '       ', 're', 'on', 'te', '\\n\n",
            "', '\\n        ', 'or', 'st', 'de', '\\n   ', 'th', 'le', ' =', 'lf', 'self',\n",
            "'me', 'al']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:280]]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww2LSbtmvy6f",
        "outputId": "a1eca441-0bc6-4989-c050-4216d6b88700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' capt', ' embedded', ' regarding', 'Bundle', '355', ' recv', ' dmp', ' vault',\n",
            "' Mongo', ' possibly', 'implementation', 'Matches']\n"
          ]
        }
      ],
      "source": [
        "print([f'{new_tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-12:]]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0HzNDTFvy6f",
        "outputId": "fbeeb8cd-c45f-4838-d7f9-712e6becf669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġs', 'ay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n",
            "'ĠWor', 'ld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 's', 'ay', '_', 'hello',\n",
            "'()', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "print(new_tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH7SkyPLvy6f",
        "outputId": "f3444b91-5a3d-4843-d667-f274e13628bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are in total 35 Python keywords.\n",
            "No, keyword `await` is not in the vocabulary\n",
            "No, keyword `finally` is not in the vocabulary\n",
            "No, keyword `nonlocal` is not in the vocabulary\n"
          ]
        }
      ],
      "source": [
        "import keyword\n",
        "\n",
        "print(f'There are in total {len(keyword.kwlist)} Python keywords.')\n",
        "for keyw in keyword.kwlist:\n",
        "    if keyw not in new_tokenizer.vocab:\n",
        "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNs6eUJlvy6g",
        "outputId": "7246436c-e67c-4565-eb94-45263ca67ca3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [05:08<00:00,  1.54s/it]\n"
          ]
        }
      ],
      "source": [
        "length = 200000\n",
        "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n",
        "    vocab_size=32768, initial_alphabet=base_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xym-wuDvy6g",
        "outputId": "4849103f-1c13-41a2-e6ef-7f5510fefb54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lineEdit', 'spik', ' BC', 'pective', 'OTA', 'theus', 'FLUSH', ' excutils',\n",
            "'00000002', ' DIVISION', 'CursorPosition', ' InfoBar']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n",
        "                reverse=False)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9faxDwUzvy6g",
        "outputId": "705c3b41-8c5a-49c0-b13d-8f108b091e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n",
            "'ĠWorld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_', 'hello', '()',\n",
            "'Ċ']\n"
          ]
        }
      ],
      "source": [
        "print(new_tokenizer_larger(python_code).tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edr6wwvNvy6g",
        "outputId": "41d09242-b6ea-420a-bdf1-a39dbc55bdd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No, keyword `nonlocal` is not in the vocabulary\n"
          ]
        }
      ],
      "source": [
        "for keyw in keyword.kwlist:\n",
        "    if keyw not in new_tokenizer_larger.vocab:\n",
        "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXLI2gBTvy6h"
      },
      "source": [
        "### Saving a Custom Tokenizer on the Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh_ats6Svy6h",
        "outputId": "eae58667-8052-4406-a458-dd61719d03b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/transformersbook/codeparrot into local empty directory.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/transformersbook/codeparrot/commit/1c284adaa3cc9f8635ae7e3377bd3739f48bc09a'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_ckpt = \"codeparrot\"\n",
        "org = \"transformersbook\"\n",
        "new_tokenizer_larger.push_to_hub(model_ckpt, organization=org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNk3v7Svy6h",
        "outputId": "a47b34a2-159d-45f4-d2d3-eba9a76e0f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n",
            "'ĠWorld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_', 'hello', '()',\n",
            "'Ċ']\n"
          ]
        }
      ],
      "source": [
        "reloaded_tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\n",
        "print(reloaded_tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irwaUAmCvy6h",
        "outputId": "a3e0564a-30ed-4c80-8488-9c4d0199a302"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/transformersbook/codeparrot-small-vocabulary into local empty directory.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/transformersbook/codeparrot-small-vocabulary/commit/0b37bed9956d95d0b79ada169f6a281e15c63381'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_tokenizer.push_to_hub(model_ckpt+ \"-small-vocabulary\", organization=org)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFW7YN3evy6i"
      },
      "source": [
        "## Training a Model from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cympURkXvy6i"
      },
      "source": [
        "### A Tale of Pretraining Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_RIoiJIvy6i"
      },
      "source": [
        "<img alt=\"Code snippet\" caption=\"An example of a Python function that could be found in our dataset\" src=\"https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter10_code-snippet.png?raw=1\" id=\"code-snippet\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwPzlPMyvy6i"
      },
      "source": [
        "#### Causal language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvMmqu_9vy6j"
      },
      "source": [
        "<img alt=\"CLM pretraining\" caption=\"In causal language modeling, the future tokens are masked and the model has to predict them; typically a decoder model such as GPT is used for such a task\" src=\"https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter10_pretraining-clm.png?raw=1\" id=\"pretraining-clm\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1bNd3BDvy6j"
      },
      "source": [
        "#### Masked language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb4nTRZ-vy6j"
      },
      "source": [
        "<img alt=\"MLM pretraining\" caption=\"In masked language modeling some of the input tokens are either masked or replaced, and the model's task is to predict the original tokens; this is the architecture underlying the encoder branch of transformer models\" src=\"https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter10_pretraining-mlm.png?raw=1\" id=\"pretraining-mlm\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa3IIPrMvy6j"
      },
      "source": [
        "#### Sequence-to-sequence training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs7FXXRfvy6j"
      },
      "source": [
        "<img alt=\"Seq2seq pretraining\" caption=\"Using an encoder-decoder architecture for a sequence-to-sequence task where the inputs are split into comment/code pairs using heuristics: the model gets one element as input and needs to generate the other one\" src=\"https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter10_pretraining-seq2seq.png?raw=1\" id=\"pretraining-seq2seq\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYbymt-Qvy6k"
      },
      "source": [
        "### Initializing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flwx3QvKvy6k"
      },
      "source": [
        "> **NOTE**: In the following code block, a large GPT-2 checkpoint is loaded into memory. On platforms like Colab and Kaggle, this can cause the instance to crash due to insufficient RAM or GPU memory. You can still run the example if you use the small checkpoint by replacing the configuration with `config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82PQjdpYvy6k",
        "outputId": "c427b33a-b3df-4074-9875-731a377c60c1",
        "colab": {
          "referenced_widgets": [
            "be84ca77ca144954af8ae4820ec6685b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be84ca77ca144954af8ae4820ec6685b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/787 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\n",
        "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\n",
        "model = AutoModelForCausalLM.from_config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU3Aqh0Yvy6k",
        "outputId": "1d10d1c0-a4fe-4925-8c1d-19fcc7779feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 (xl) size: 1529.6M parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXy1GYSRvy6k"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True,\n",
        "                      organization=org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-TzNfEhvy6l"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\n",
        "model_small = AutoModelForCausalLM.from_config(config_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNCpjFIPvy6l",
        "outputId": "6311c758-c4e8-49fd-80ee-f74f6c4a01b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 size: 111.0M parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV5cQ6DCvy6l",
        "outputId": "773dc58f-6425-4fa9-b4e7-3b16fdfc9121"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/transformersbook/codeparrot-small into local empty directory.\n"
          ]
        }
      ],
      "source": [
        "model_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True,\n",
        "                            organization=org)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmOoMxMavy6l"
      },
      "source": [
        "### Implementing the Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4PQvvy9vy6l"
      },
      "source": [
        "<img alt=\"Preprocessing for CLM\" caption=\"Preparing sequences of varying length for causal language modeling by concatenating several tokenized examples with an EOS token  before chunking them\" src=\"https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter10_preprocessing-clm.png?raw=1\" id=\"preprocessing-clm\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlbiMJySvy6m",
        "outputId": "a9da31a5-3174-451c-8b3e-13b5d8fff2d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/500 [00:00<01:16,  6.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2605 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 500/500 [00:04<00:00, 122.59it/s]\n"
          ]
        }
      ],
      "source": [
        "examples, total_characters, total_tokens = 500, 0, 0\n",
        "dataset = load_dataset('transformersbook/codeparrot-train', split='train',\n",
        "                       streaming=True)\n",
        "\n",
        "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
        "    total_characters += len(example['content'])\n",
        "    total_tokens += len(tokenizer(example['content']).tokens())\n",
        "\n",
        "characters_per_token = total_characters / total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4z6wqw-vy6m",
        "outputId": "c9c2fb7c-39d6-441e-feb8-a9b16e6f835b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.6233025034779565\n"
          ]
        }
      ],
      "source": [
        "print(characters_per_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAqY2Qaevy6m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "    \n",
        "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
        "                 num_of_sequences=1024, chars_per_token=3.6):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.concat_token_id = tokenizer.eos_token_id\n",
        "        self.dataset = dataset\n",
        "        self.seq_length = seq_length\n",
        "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
        "    \n",
        "    def __iter__(self):\n",
        "        iterator = iter(self.dataset)\n",
        "        more_examples = True\n",
        "        while more_examples:\n",
        "            buffer, buffer_len = [], 0\n",
        "            while True:\n",
        "                if buffer_len >= self.input_characters:\n",
        "                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n",
        "                    print(m)\n",
        "                    break\n",
        "                try:\n",
        "                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
        "                    print(m)\n",
        "                    buffer.append(next(iterator)[\"content\"])\n",
        "                    buffer_len += len(buffer[-1])\n",
        "                except StopIteration:\n",
        "                    iterator = iter(self.dataset)\n",
        "\n",
        "            all_token_ids = []\n",
        "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
        "            for tokenized_input in tokenized_inputs['input_ids']:\n",
        "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
        "            \n",
        "            for i in range(0, len(all_token_ids), self.seq_length):\n",
        "                input_ids = all_token_ids[i : i + self.seq_length]\n",
        "                if len(input_ids) == self.seq_length:\n",
        "                    yield torch.tensor(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA7JMkp_vy6n",
        "outputId": "be651acb-f75f-4dfe-d9e2-764e744723a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fill buffer: 0<36864\n",
            "Fill buffer: 3311<36864\n",
            "Fill buffer: 9590<36864\n",
            "Fill buffer: 22177<36864\n",
            "Fill buffer: 25530<36864\n",
            "Fill buffer: 31098<36864\n",
            "Fill buffer: 32232<36864\n",
            "Fill buffer: 33867<36864\n",
            "Buffer full: 41172>=36864\n",
            "Lengths of the sequences: [1024, 1024, 1024, 1024, 1024]\n"
          ]
        }
      ],
      "source": [
        "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
        "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n",
        "                                                num_of_sequences=10)\n",
        "dataset_iterator = iter(constant_length_dataset)\n",
        "\n",
        "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
        "print(f\"Lengths of the sequences: {lengths}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dKRvDBwvy6n"
      },
      "source": [
        "### Defining the Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEE-rQxuvy6n"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# Commented parameters correspond to the small model\n",
        "config = {\"train_batch_size\": 2, # 12\n",
        "          \"valid_batch_size\": 2, # 12\n",
        "          \"weight_decay\": 0.1,\n",
        "          \"shuffle_buffer\": 1000,\n",
        "          \"learning_rate\": 2e-4, # 5e-4\n",
        "          \"lr_scheduler_type\": \"cosine\",\n",
        "          \"num_warmup_steps\": 750, # 2000\n",
        "          \"gradient_accumulation_steps\": 16, # 1\n",
        "          \"max_train_steps\": 50000, # 150000\n",
        "          \"max_eval_steps\": -1,\n",
        "          \"seq_length\": 1024,\n",
        "          \"seed\": 1,\n",
        "          \"save_checkpoint_steps\": 50000} # 15000\n",
        "\n",
        "args = Namespace(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvLFdp84vy6n"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import logging\n",
        "import wandb\n",
        "\n",
        "def setup_logging(project_name):\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n",
        "        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
        "        logging.StreamHandler()])\n",
        "    if accelerator.is_main_process: # We only want to set up logging once\n",
        "        wandb.init(project=project_name, config=args)\n",
        "        run_name = wandb.run.name\n",
        "        tb_writer = SummaryWriter()\n",
        "        tb_writer.add_hparams(vars(args), {'0': 0})\n",
        "        logger.setLevel(logging.INFO)\n",
        "        datasets.utils.logging.set_verbosity_debug()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        tb_writer = None\n",
        "        run_name = ''\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "    return logger, tb_writer, run_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLHkmCTfvy6o"
      },
      "outputs": [],
      "source": [
        "def log_metrics(step, metrics):\n",
        "    logger.info(f\"Step {step}: {metrics}\")\n",
        "    if accelerator.is_main_process:\n",
        "        wandb.log(metrics)\n",
        "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CEHCz12vy6o",
        "outputId": "0abf66e4-8a5a-4137-a87f-792a3c6b08d6",
        "colab": {
          "referenced_widgets": [
            "328dc6d7d05c452e8d8e2cab5b4b9c4e"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "328dc6d7d05c452e8d8e2cab5b4b9c4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Check remote data files:   0%|          | 0/183 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration codeparrot-train-938ce362e6f661b1\n",
            "Using custom data configuration codeparrot-valid-29167601d8e69487\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "def create_dataloaders(dataset_name):\n",
        "    train_data = load_dataset(dataset_name+'-train', split=\"train\",\n",
        "                              streaming=True)\n",
        "    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\n",
        "                                    seed=args.seed)\n",
        "    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\",\n",
        "                              streaming=True)\n",
        "    \n",
        "    train_dataset = ConstantLengthDataset(tokenizer, train_data,\n",
        "                                          seq_length=args.seq_length)\n",
        "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data,\n",
        "                                          seq_length=args.seq_length)\n",
        "    \n",
        "    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
        "    eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n",
        "    return train_dataloader, eval_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuC_HiiPvy6o"
      },
      "outputs": [],
      "source": [
        "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
        "    params_with_wd, params_without_wd = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if any(nd in n for nd in no_decay):\n",
        "            params_without_wd.append(p)\n",
        "        else:\n",
        "            params_with_wd.append(p)\n",
        "    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n",
        "            {'params': params_without_wd, 'weight_decay': 0.0}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo35RyPLvy6o"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch, labels=batch)\n",
        "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
        "        losses.append(accelerator.gather(loss))\n",
        "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
        "    loss = torch.mean(torch.cat(losses))\n",
        "    try:\n",
        "\t\tperplexity = torch.exp(loss)\n",
        "\texcept OverflowError:\n",
        "\t\tperplexity = torch.tensor(float(\"inf\"))\n",
        "    return loss.item(), perplexity.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20kWu-bmvy6p"
      },
      "outputs": [],
      "source": [
        "set_seed(args.seed)\n",
        "\n",
        "# Accelerator\n",
        "accelerator = Accelerator()\n",
        "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
        "\n",
        "# Logging\n",
        "logger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\n",
        "logger.info(accelerator.state)\n",
        "\n",
        "# Load model and tokenizer\n",
        "if accelerator.is_main_process:\n",
        "    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
        "\n",
        "# Load dataset and dataloader\n",
        "train_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n",
        "\n",
        "# Prepare the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
        "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
        "                             num_warmup_steps=args.num_warmup_steps,\n",
        "                             num_training_steps=args.max_train_steps,)\n",
        "def get_lr():\n",
        "    return optimizer.param_groups[0]['lr']\n",
        "\n",
        "# Prepare everything with our `accelerator` (order of args is not important)\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "# Train model\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "for step, batch in enumerate(train_dataloader, start=1):\n",
        "    loss = model(batch, labels=batch).loss\n",
        "    log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,\n",
        "                       'steps': completed_steps, 'loss/train': loss.item()})\n",
        "    loss = loss / args.gradient_accumulation_steps\n",
        "    accelerator.backward(loss)\n",
        "    if step % args.gradient_accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        completed_steps += 1\n",
        "    if step % args.save_checkpoint_steps == 0:\n",
        "        logger.info('Evaluating and saving model checkpoint')\n",
        "        eval_loss, perplexity = evaluate()\n",
        "        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
        "        accelerator.wait_for_everyone()\n",
        "        unwrapped_model = accelerator.unwrap_model(model)\n",
        "        if accelerator.is_main_process:\n",
        "            unwrapped_model.save_pretrained(\"./\")\n",
        "            hf_repo.push_to_hub(commit_message=f'step {step}')\n",
        "        model.train()\n",
        "    if completed_steps >= args.max_train_steps:\n",
        "        break\n",
        "\n",
        "# Evaluate and save the last checkpoint\n",
        "logger.info('Evaluating and saving model after training')\n",
        "eval_loss, perplexity = evaluate()\n",
        "log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "if accelerator.is_main_process:\n",
        "    unwrapped_model.save_pretrained(\"./\")\n",
        "    hf_repo.push_to_hub(commit_message=f'final model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iNc-qQkvy6p"
      },
      "source": [
        "<img alt=\"DDP\" caption=\"Illustration of the processing steps in DDP with four GPUs\" src=\"https://github.com/rickiepark/nlp-with-transformers/blob/main/images/chapter10_ddp.png?raw=1\" id=\"ddp\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn4urjcVvy6p"
      },
      "source": [
        "### The Training Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N6jAyHKvy6p"
      },
      "source": [
        "## Results and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOwUMsHmvy6p",
        "outputId": "843218a2-355c-4d34-a238-838f74fdc7a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-20 18:29:01.107727: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2021-10-20 18:29:01.107759: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "model_ckpt = 'transformersbook/codeparrot-small'\n",
        "generation = pipeline('text-generation', model=model_ckpt, device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni2bSy9nvy6q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import set_seed \n",
        "\n",
        "def first_block(string):\n",
        "    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n",
        "\n",
        "def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n",
        "    set_seed(seed)\n",
        "    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\n",
        "                  \"do_sample\":True,}\n",
        "    code_gens = generation(prompt, num_return_sequences=num_completions, \n",
        "                            max_length=max_length, **gen_kwargs)\n",
        "    code_strings = []\n",
        "    for code_gen in code_gens:\n",
        "        generated_code = first_block(code_gen['generated_text'][len(prompt):])\n",
        "        code_strings.append(generated_code)\n",
        "    print(('\\n'+'='*80 + '\\n').join(code_strings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AQrvODqvy6q",
        "outputId": "f0db9791-2a7d-4ae5-8ec6-f9f30520622a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    return math.sqrt(a * b)\n",
            "================================================================================\n",
            "\n",
            "    return a * b / 2.0\n",
            "================================================================================\n",
            "\n",
            "    return a * b\n",
            "================================================================================\n",
            "\n",
            "    return a * b / a\n"
          ]
        }
      ],
      "source": [
        "prompt = '''def area_of_rectangle(a: float, b: float):\n",
        "    \"\"\"Return the area of the rectangle.\"\"\"'''\n",
        "complete_code(generation, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z_8mv6Uvy6q",
        "outputId": "9057699f-b2d9-4ce2-e989-72997e988f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    if not html:\n",
            "        return []\n",
            "    return [url for url in re.findall(r'<a href=\"(/[^/]+/[^\"]+?)\">', html)]\n",
            "================================================================================\n",
            "\n",
            "    return [url for url in re.findall(r'<a href=\"(.*?)\"', html)\n",
            "            if url]\n",
            "================================================================================\n",
            "\n",
            "    return [url for url in re.findall(r'<a href=\"(/.*)\",', html)]\n",
            "================================================================================\n",
            "\n",
            "    return re.findall(r'<a href=\"(.*?)\" class=\"url\"[^>]*>', html)\n"
          ]
        }
      ],
      "source": [
        "prompt = '''def get_urls_from_html(html):\n",
        "    \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\n",
        "complete_code(generation, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLv_0kGIvy6q",
        "outputId": "0909bfa0-5365-4a6c-ed0e-f458b41e2343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://github.com/huggingface/transformers | /allenai | /facebook |\n",
            "/asteroid-team | /google | /amazon | /speechbrain | /microsoft | /grammarly |\n",
            "/models | /inference-api | /distilbert-base-uncased |\n",
            "/dbmdz/bert-large-cased-finetuned-conll03-english |\n",
            "https://huggingface.co/transformers | https://arxiv.org/abs/1811.06031 |\n",
            "https://arxiv.org/abs/1803.10631 | https://transformer.huggingface.co/ | /coref\n",
            "| https://medium.com/huggingface/distilbert-8cf3380435b5\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def get_urls_from_html(html):\n",
        "    return [url for url in re.findall(r'<a href=\"(.*?)\"', html) if url]\n",
        "\n",
        "print(\" | \".join(get_urls_from_html(requests.get('https://hf.co/').text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44uG4TjNvy6r"
      },
      "source": [
        "> **NOTE**: In the following code block, a large GPT-2 checkpoint is loaded into memory. On platforms like Colab and Kaggle, this can cause the instance to crash due to insufficient RAM or GPU memory. You can still run the example if you replace the large model with the small one by using `model_ckpt = \"transformersbook/codeparrot-small\"`.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7431Eltjvy6r",
        "outputId": "9e19c60e-6d0b-4687-a6d2-fc5f6db68289"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    return np.mean(a)\n",
            "================================================================================\n",
            "\n",
            "    return np.mean(a)\n",
            "================================================================================\n",
            "\n",
            "    return np.mean(a)\n",
            "================================================================================\n",
            "\n",
            "    return np.mean(a)\n"
          ]
        }
      ],
      "source": [
        "model_ckpt = 'transformersbook/codeparrot'\n",
        "generation = pipeline('text-generation', model=model_ckpt, device=0)\n",
        "\n",
        "prompt = '''# a function in native python:\n",
        "def mean(a):\n",
        "    return sum(a)/len(a)\n",
        "\n",
        "# the same function using numpy:\n",
        "import numpy as np\n",
        "def mean(a):'''\n",
        "complete_code(generation, prompt, max_length=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8pLxjOTvy6r",
        "outputId": "e828a56c-4415-43ec-e6ae-71fa229972e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "reg = DummyRegressor()\n",
            "\n",
            "forest = RandomForestClassifier(n_estimators=20)\n",
            "\n",
            "forest.fit(X, y)\n",
            "================================================================================\n",
            "\n",
            "clf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')\n",
            "clf.fit(X, y)\n",
            "================================================================================\n",
            "\n",
            "clf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)\n",
            "clf.fit(X, y)\n",
            "================================================================================\n",
            "\n",
            "clf = RandomForestClassifier(n_estimators=20)\n",
            "clf.fit(X, y)\n"
          ]
        }
      ],
      "source": [
        "prompt = '''X = np.random.randn(100, 100)\n",
        "y = np.random.randint(0, 1, 100)\n",
        "\n",
        "# fit random forest classifier with 20 estimators'''\n",
        "complete_code(generation, prompt, max_length=96)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyvV97VIvy6r"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igKKMgKRvy6r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7097ae8aaa84c59ba42093dde5d1703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_742bfe3ce6ca4fde9769ad3996b4b5a0",
              "IPY_MODEL_ef8b2387a73a4b3b875563fdd3a3d390",
              "IPY_MODEL_9bf683804d314145aee648f38b1eea63"
            ],
            "layout": "IPY_MODEL_815c8ebf4adf41b2a605846b7329d50f"
          }
        },
        "742bfe3ce6ca4fde9769ad3996b4b5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e0352d6c1f4978b779a01da7d10a01",
            "placeholder": "​",
            "style": "IPY_MODEL_68e06296b5df4202afd7c3d796f29a55",
            "value": "Resolving data files: 100%"
          }
        },
        "ef8b2387a73a4b3b875563fdd3a3d390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f159b1c93b584542863fe41e3462e5b8",
            "max": 186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29e9e5c12d564fb38e28a7ac9bd11f21",
            "value": 186
          }
        },
        "9bf683804d314145aee648f38b1eea63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5a860fce21f4a629f1ccc8fc03931cb",
            "placeholder": "​",
            "style": "IPY_MODEL_8df2464a04e341bfa1ba8b5ab7a3b93a",
            "value": " 186/186 [00:00&lt;00:00, 3128.02it/s]"
          }
        },
        "815c8ebf4adf41b2a605846b7329d50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e0352d6c1f4978b779a01da7d10a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68e06296b5df4202afd7c3d796f29a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f159b1c93b584542863fe41e3462e5b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e9e5c12d564fb38e28a7ac9bd11f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5a860fce21f4a629f1ccc8fc03931cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df2464a04e341bfa1ba8b5ab7a3b93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}